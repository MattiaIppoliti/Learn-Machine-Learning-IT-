{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduzione a Jupyter e Python per il Machine Learning (*pandas*, *numpy* e *scikit-learn*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installazione di Anaconda\n",
    "\n",
    "Scaricare *l'installer* di **Anaconda Individual Edition** adatta alla propria macchina (32 o 64 bit).<br>\n",
    "Installare Anaconda in una directory il cui path non contenga spazi o caratteri unicode.<br>\n",
    "Avviare Anaconda/Jupyter con **Anaconda Navigator** oppure l'**Anaconda Prompt**, entrambi disponibili dal menù.<br>\n",
    "[In Windows, non confondere l'Anaconda Prompt con il terminale di sistema del DOS.]\n",
    "\n",
    "Per Windows, vedi [qui](https://docs.anaconda.com/anaconda/install/windows/).<br>\n",
    "Per Mac, vedi [qui](https://docs.anaconda.com/anaconda/install/mac-os/).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I principali package python per Machine Learning\n",
    "* **scikit-learn**: la libreria più importante per il ML predittivo (ed anche descrittivo); si importano i vari moduli via via che servono;\n",
    "* **numpy** (np): libreria per il calcolo scientifico; contiene molte funzioni matematiche di alto livello per l'algebra lineare (ad es. prodotti scalari tra vettore e matrice); contiene un generatore di numeri pseudo-casuali; permette di creare array multi-dimensionali con operazioni vettorizzate; nato nel 2006 dall'unione di due precedenti package numerici; si appoggia a veloci librerie C e Fortran (BLAS e LAPACK); è considerato la versione Python di Matlab;\n",
    "* **scipy** (sp): libreria che estende numpy con 60+ funzioni statistiche e matematiche; permette la gestione di matrici sparse;\n",
    "* **pandas** (pd): libreria per importare, gestire e manipolare i data frame in vari formati (serie, dataframe, ecc); per estrarre una parte dei dati, unire due dataset; contiene anche alcune funzioni statistiche di base; è costruito sopra numpy; i due oggetti principali di *pandsa* sono: il *data.frame* e le *series*;\n",
    "* **matplotlib**(plt): libreria per creare grafici a partire dei dati; pandas e seaborn sono dei wrapper di matplotlib, che permette\n",
    "un controllo più fine; non è di utilizzo immediato; non è oggetto di questo corso, che comunque includerà alcuni suoi utilizzi.\n",
    "* **seaborn**(sn): altra libreria grafica. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controllo versioni dei package necessari al corso\n",
    "* python        >= 3.5\n",
    "* scikit-learn  >= 0.23.2 \n",
    "* pandas        >= 0.18.0 \n",
    "* numpy         >= 1.11.0\n",
    "* scipy         >= 0.17.0\n",
    "* matplotlib    >= 1.5.1\n",
    "* joblib        >= 0.11\n",
    "* seaborn?\n",
    "\n",
    "Verifica dalla scheda \"Environments\" di Anaconda Navigator,  <br>\n",
    "oppure da Jupyter Notebook così (uno dei molti modi...): <br>\n",
    "[vedi stack overflow (so) 20180543 oppure 710609].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# check versione python (identica in Linux o Win10):\n",
    "# da un anaconda prompt: python --version\n",
    "# da terminale IPython oppure da jupyter:\n",
    "import sys  \n",
    "print(sys.version)  # 'sys.version' provides a string containing the version number of the Python interpreter plus additional \n",
    "                    # information on the build number and compiler used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check versione package (fattibile se python >= 3.8):\n",
    "from importlib_metadata import version\n",
    "version ('scikit-learn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check versione package (se Python < 3.8): qui, come esempio, di numpy:\n",
    "import pkg_resources\n",
    "pkg_resources.get_distribution('numpy').version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note su Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python2 vs Python3\n",
    "Python3, anche chiamato Py3k o Python3000, è stato rilasciato nel 2008.<br>\n",
    "Vediamo alcune differenze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in python2 'print' era un'istruzione, in python3 è una funzione, e quindi obbligatoriamente con le parentesi tonde.\n",
    "print(\"abc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in python3:\n",
    "3/2     # --> 1.5, mentre in python2 il risultato era 1, e solo 3./2 dava risultato 1.5\n",
    "3.//2.  # sia con python3 che python2 il risultato è 1 (cioè, con arrotondamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# il check della versione di Python si può fare da anaconda prompt con: 'python -V'\n",
    "# se la versione installata risulta la 3, si può allora verificare la (eventuale) con-presenza di python2 con: 'python2 - V'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'allocazione della memoria in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[L'allocazione di memoria in Python](CvsPythonMemoryAllocation.png) \n",
    "\n",
    "**a sx**: In C la variabile x DEVE essere dichiarata, il compilatore crea uno spazio in memoria per essa (qui come intero).\n",
    "Le due assegnazioni riportate non danno problemi; se si assegnasse invece una stringa ad x, il compilatore darebbe errore.<br>\n",
    "**a dx**: x è solo un PUNTATORE ad un oggetto in memoria. Le tre assegnazioni riportate creano 3 differenti oggetti in <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poichè dunque le variabili Python sono puntatori ad oggetti di memoria, dobbiamo distinguere tra variabili che puntano allo\n",
    "# STESSO oggetto e variabili che puntano ad oggetti DIFFERENTI ma UGUALI.\n",
    "# Un esempio:\n",
    "a = [1,2]\n",
    "b = a       # cioè, b punta alla stessa locazione di memoria di a\n",
    "c = [1,2]\n",
    "b.append(3) # metodo che aggiunge un elemento alla lista\n",
    "print(\"a =\",a)\n",
    "print(\"b =\",b)\n",
    "print(\"c =\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Al contrario, in R (od in C) si otterrebbe:\n",
    "# a = [1,2]\n",
    "# b = [1,2,3]\n",
    "# c = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ha un \"counter of pointers\" ed un \"garbage collector\", e cancella automaticamente dalla memoria gli oggetti\n",
    "# che non sono puntati da nessuna variabile. Nella precedente figura, dopo la terza assegnazione, i primi due puntatori \n",
    "# sono cancellati automaticamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretazione / Compilazione e Tipizzazione\n",
    "Da Wikipedia IT (\"python\"):<br>\n",
    "1.<br>\n",
    "Il controllo dei tipi è forte (strong typing) e viene eseguito in runtime (dynamic typing): una variabile è un contenitore a cui viene associata un'etichetta (il nome) che può essere associata a diversi contenitori anche di tipo diverso durante il suo tempo di vita. Fa parte di Python un sistema garbage collector per liberazione e recupero automatico della memoria di lavoro.<br>\n",
    "2.<br>\n",
    "Sebbene Python venga in genere considerato un linguaggio interpretato, in realtà il codice sorgente non viene convertito direttamente in linguaggio macchina. Infatti passa prima da una fase di pre-compilazione in bytecode, che viene quasi sempre riutilizzato dopo la prima esecuzione del programma, evitando così di reinterpretare ogni volta il sorgente e migliorando le prestazioni. Inoltre è possibile distribuire programmi Python direttamente in bytecode, saltando totalmente la fase di interpretazione da parte dell'utilizzatore finale e ottenendo programmi Python a sorgente chiuso.<br>\n",
    "3.<br>\n",
    "Se paragonato ai linguaggi compilati statically typed, come ad esempio il C, la velocità di esecuzione non è uno dei punti di forza di Python, specie nel calcolo matematico. Inoltre, il programma si basa unicamente su un core, ed il multi-threading è presente al solo livello astratto.<br>\n",
    "4.<br>\n",
    "Essendo Python a tipizzazione dinamica, tutte le variabili sono in realtà puntatori a oggetto. Per esempio se a una variabile è assegnato un valore numerico intero, subito dopo può essere assegnata una stringa o una lista. Gli oggetti sono invece dotati di tipo.<br>\n",
    "5.<br>\n",
    "Python prevede un moderato controllo dei tipi al momento dell'esecuzione, ovvero runtime.<br>\n",
    "6.<br>\n",
    "Per la natura fortemente imprevedibile, i linguaggi a tipizzazione dinamica sono spesso anche interpretati, in quanto l'interprete costituisce un ambiente di esecuzione sicuro, in grado di assecondare tutti i cambiamenti di tipo delle variabili.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alcune tipiche difficoltà (concettuali) della OOP\n",
    "[copiato vs. assegnato](https://medium.com/@thawsitt/assignment-vs-shallow-copy-vs-deep-copy-in-python-f70c2f0ebd86)<br>\n",
    "[l'argomento inplace di pandas](https://stackoverflow.com/questions/43893457/understanding-inplace-true)\n",
    "\n",
    "In generale, *metodi* e *funzioni* in python sono cose un pò diverse: per semplicità diciamo che un metodo è una funzione che è \n",
    "strettamente collegata con un \"oggetto\" (ad es. un dataframe pandas, oppure una lista) e che ha accesso ai\n",
    "suoi dati; una funzione deve invece avere specificati i dati sui quali agire tramite i suoi argomenti in parentesi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note su Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Il funzionamento del *kernel*\n",
    "Ogni notebook jupyter ha un **kernel** associato, che è il lato back-end che esegue il codice.<br>\n",
    "Il notebook ed il relativo kernel sono disaccoppiati, nel senso che l'uno può essere attivo senza l'altro.<br>\n",
    "Le celle di tipo *markdown* possono essere eseguite anche se il kernel del notebook non è attivo. Al contrario, l'esecuzione di una cella di tipo codice è possibile solo se il kernel è attivo.<br>\n",
    "Il kernel si attiva / disattiva dal tab *kernel* (nel menù in alto del notebook), oppure anche dal menù di Jupyter.<br>\n",
    "Se si esegue una cella codice (con shift+enter) ed il kernel del notebook non è attivo, le parentesi quadre della cella in esecuzione contengono per tempo indefinito un asterisco e quindi non si otterrà mai l'output della cella sinchè non si riattivi il kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Celle markdown\n",
    "\n",
    "**heading**:\n",
    "- indice di primo livello: #\n",
    "- indice di secondo livello: ##\n",
    "- ecc\n",
    "\n",
    "**punti elenco**: '-' oppure asterisco per ogni punto \n",
    "* a\n",
    "* b\n",
    "* ecc\n",
    "\n",
    "**testo in maiuscolo** \n",
    "\n",
    "*testo in corsivo*\n",
    "\n",
    "<u>testo sottolineato</u>\n",
    "\n",
    "[nome-link](link) (filepath, url, ecc)\n",
    "\n",
    "> evidenza a sx\n",
    "\n",
    "**toggle tipo cella**:\n",
    "* code --> markdown: esc+m\n",
    "* markdown --> code: esc+y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temi, font e size dei caratteri\n",
    "[utile link - \"method two\"](https://towardsdatascience.com/7-essential-tips-for-writing-with-jupyter-notebook-60972a1a8901#4f95)<br>\n",
    "[VS code](https://code.visualstudio.com/download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View > Toggle Line Numbers (comodo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOC (Table of Contents) in Jupyter\n",
    "[Come creare una TOC in Jupyter](https://stackoverflow.com/questions/21151450/how-can-i-add-a-table-of-contents-to-a-jupyter-jupyterlab-notebook) <br>\n",
    "[Cos'è JupyterLab](https://stackoverflow.com/questions/50982686/what-is-the-difference-between-jupyter-notebook-and-jupyterlab) <br>\n",
    "[Come installare le jupyter_contrib_nbextensions](https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/install.html) <br>\n",
    "[Come installare PIP in Windows](https://phoenixnap.com/kb/install-pip-windows) <br>\n",
    "[Come installare PIP in Linux](https://www.tecmint.com/install-pip-in-linux/) <br>\n",
    "\n",
    "Occorre chiudere anaconda e poi riaprirla per vedere il tab estensioni di Jupyter. <br>\n",
    "Occorre chiudere i notebook e riaprirli per vedere il ToC.\n",
    "Il ToC è visualizzato a sinistra, ma è folttante e si può spostare in qualsiasi parte dello screen.\n",
    "Attenzione: le voci del ToC sono attive (funzionanti) solo se le rispettive celle markdown che le definiscono <br>\n",
    "sono in quel momento in command mode (blu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help in Jupyter\n",
    "* 'dir()' shows the possible object, method and function calls available to that object.\n",
    "* 'help()' function for imported modules \n",
    "* tab completion\n",
    "* 'shift + tab' dentro le parentesi di una funzione (non di un metodo!) fornisce la documentazione dei suoi argomenti\n",
    "* nome_modulo? help on the imported module\n",
    "* nome_variabile? fornisce info sulla variable\n",
    "* help on-line: https://jupyter.readthedocs.io/en/latest/\n",
    "\n",
    "[Come avere aiuto in Jupyter](https://problemsolvingwithpython.com/02-Jupyter-Notebooks/02.07-Getting-Help-in-a-Jupyter-Notebook/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigare su e giù nel notebook\n",
    "**Se le nb extensions sono installate** si possono usare i consueti tasti: *home* o l'equivalente freccia verso l'alto a sx (inizio nb), <br>\n",
    "*end* (fine nb), *page down*, *page up*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installazione di package\n",
    "In Anaconda Navigator la maggior parte dei package sono già installati (tutti per questo corso). <br>\n",
    "Da terminale IPython o da Jupyter: con la funzione pip, ad esempio:\n",
    "python -m pip install numpy <br>\n",
    "[Installing Python Packages from a Jupyter Notebook](https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output celle in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output di cella\n",
    "a = 5\n",
    "b = 10\n",
    "a\n",
    "b # --> solo l'ultimo output è riportato sotto la cella (a meno che si usi la 'print'). \n",
    "  #     Nelle altre celle si può usare 'print'(nome_oggetto) o semplicemente nome_oggetto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart & Clear Output\n",
    "All'apertura di un notebook esistente, jupyter visualizza tutti gli output della sessione precedente. Ciò in alcuni casi è comodo: il notebook può essere letto ed analizzato senza doverlo rieseguire. In altri casi, invece, ciò può essere fuorviante, ad esempio se si apre per la prima volta un notebook creato da un'altra persona od anche da noi su un altro PC. In questi casi, infatti, è bene rieseguire il codice su *questo* PC, per **verificarne la corretta esecuzione**.<br>\n",
    "Si fa così:\n",
    "* dal menù del notebook (da non confondere con il menù di Jupyter) premere *kernel*\n",
    "* premere quindi *Restart & Clear Output*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerazione celle\n",
    "Dopo l'esecuzione di una cella, Jupyter riporta tra parentesi quadre il numero di esecuzione e di output <u>all'interno della sessione</u>.Tale numerazione <u>non</u> segue l'ordine sequenziale del notebook, ma appunto quello di esecuzione e di display. Questi ultimi possono differire in quanto molte celle non hanno output e dunque la loro esecuzione incrementa solo il contatore di esecuzione.<br>\n",
    "Se il kernel non è attivo, in parentesi quadra è riportato un asterisco, che ricorda appunto la <u>non</u> esecuzione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Alcuni comandi \"magic\"\n",
    "Per interagire con il sistema operativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%quickref   # IPython -- An enhanced Interactive Python - Quick Reference Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista dei comandi \"magic\"\n",
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print working directory (pwd)\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list files & subdirectories in directory\n",
    "%ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list just subdirectories\n",
    "%ddir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos # la lista delle variabili; è come il variable inspector ma non è aggiornata in tempo reale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit (10==20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da so 29280470:<br><br>\n",
    "**%timeit** is an ipython magic function, which can be used to time a particular piece of code (A single execution statement, or a single method).<br><br>\n",
    "From the docs:<br>\n",
    "%timeit<br>\n",
    "Time execution of a Python statement or expression<br>\n",
    "Usage, in line mode:<br>\n",
    "    %timeit [-n<N> -r<R> [-t|-c] -q -p<P> -o] statement<br>\n",
    "To use it, for example if we want to find out whether using xrange is any faster than using range, you can simply do:<br>\n",
    "\n",
    "In [1]: %timeit for _ in range(1000): True<br>\n",
    "10000 loops, best of 3: 37.8 µs per loop<br>\n",
    "\n",
    "In [2]: %timeit for _ in xrange(1000): True<br>\n",
    "10000 loops, best of 3: 29.6 µs per loop<br>\n",
    "And you will get the timings for them.<br>\n",
    "\n",
    "The major advantage of %timeit are:<br>\n",
    "* that you don't have to import timeit.timeit from the standard library, and run the code multiple times to figure out which is the better approach.\n",
    "* %timeit will automatically calculate number of runs required for your code based on a total of 2 seconds execution window.\n",
    "\n",
    "You can also make use of current console variables without passing the whole code snippet as in case of timeit.timeit to built the variable that is built in an another environment that timeit works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativamente ([stackoverflow](https://stackoverflow.com/questions/32565829/simple-way-to-measure-cell-execution-time-in-ipython-notebook)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\"the code you want to test stays here\"\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oppure:\n",
    "import time\n",
    "start_time = time.time()\n",
    "c=3 # il codice\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trovare una stringa\n",
    "* usare la funzione del browser (ctrl-F con Win10-Chrome, ctrl-G per avanzare nella sequenza)\n",
    "* 'find and replace' (F in command mode)\n",
    "* installare le estensioni di Jupyter (da un prompt Anaconda3): <br> \n",
    "> 'pip install jupyter_contrib_nbextensions'<br>\n",
    "> 'jupyter contrib nbextension install --user'<br>\n",
    "> even after enabling nbextensions, from the notebooks tree page, you need to activate it on your Jupyter notebook <br>\n",
    "> so 49647705, varie risposte. <br>\n",
    "* usare JupyterLab\n",
    "\n",
    "Vedi anche [qui, ](https://stackoverflow.com/questions/35119831/ipython-notebook-keyboard-shortcut-search-for-text)\n",
    "[qui](https://towardsdatascience.com/12-jupyter-notebook-extensions-that-will-make-your-life-easier-e0aae0bd181)\n",
    "[ e qui](https://towardsdatascience.com/jupyter-notebook-extensions-517fa69d2231)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La gestione delle variabili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos # la lista (stesse info del 'variable inspector' ma non dinamico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "%whos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del nome_variabile\n",
    "del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo scope (l'ambito) delle variabili è (al massimo) il solo notebook aperto, cioè a meno di funzioni che definiscono variabili solo locali.<br>\n",
    "Le variabili definite in altri notebook non sono qui visibili."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chiudere Jupyter\n",
    "dalla homepage:\n",
    "* 'logout' chiude i kernel aperti;\n",
    "* 'quit' chiude <u>anche</u> i notebook aperti\n",
    "\n",
    "dal singolo notebook:\n",
    "* chiusura finestra browser --> lascia il kernel attivo (lo si può chiudere dalla homepage di Jupyter --> tab \"Running\")\n",
    "* 'logout'                  --> chiude anche il kernel. Alla prossima riapertura è richiesta la pwd od un token casuale.\n",
    "\n",
    "NB. I comandi di Jupyter NON agiscono (non possono agire) sulle finestre del browser.\n",
    "\n",
    "Vedi anche so 10162707.\n",
    "\n",
    "https://stackoverflow.com/questions/10162707/how-to-close-ipython-notebook-properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Il  Variable Inspector\n",
    "Attivazione:\n",
    "* passo 1: chiudere jupyter notebook.\n",
    "* passo 2: da anaconda prompt: \n",
    "    * 'pip install jupyter_contrib_nbextensions' (se nbextensions NON ancora installate).\n",
    "    * riattivare jupyter notebook.\n",
    "* passo 3: dal tab 'Nbextensions' della Home Page di Jupyter Notebook, che vi dovrebbe ora essere comparso in alto:\n",
    "    * fare check sul box 'Variable Inspector'\n",
    "    * e poi attivarlo con 'enable'.\n",
    "\n",
    "<br>\n",
    "(so 37718907)\n",
    "\n",
    "Il Variable Inspector permette anche di cancellare un oggetto (tramite la x iniziale sulla riga dell'oggetto stesso), come comoda alternativa alla funzione *del(oggetto)* da una cella di tipo *code*.<br>\n",
    "\n",
    "Gli oggetti del Variable Inspector possono anche essere ordinati. L'ordinamento dei nomi riporta prima i nomi in maiuscolo (ordinati) e poi in minuscolo (ordinati).<br>\n",
    "\n",
    "Con il comando magico %whos si ottiene la stessa lista di variabili, con le stesse informazioni, ma non aggiornata dinamicamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestione dei percorsi dei file\n",
    "Tramite il package 'os', che gestisce l'interazione con il sottostante sistema operativo, ed in particolare la sua funzione 'path'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzionamento cross-notebook\n",
    "copy & paste di celle (intere) tra notebook non funziona. il copy&paste dal codice della cella in edit mode, invece, è a livello di sistema <br>\n",
    "operativo, e dunque funziona anche tra differenti notebook.<br>\n",
    "**le variabili sono locali al singolo nb**<br>\n",
    "la ricerca di stringhe su vari notebook è forse fattibile da Anaconda prompt o da JupyterLab (so 60906285)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistenza degli oggetti in memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gli oggetti creati in memoria persistono alla chiusura del notebook, purchè non si esca da Jupyter.\n",
    "E' verificabile con il Variable Inspector.<br><br>\n",
    "Ciò può essere fonte di problemi. Supponiamo che un notebook esegua in questo ordine le due seguenti celle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supponiamo poi che, in un secondo momento, per errore si inserisca una terza cella *axb* tra le due, cioè nella seguente sequenza: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a*b  # funziona, perchè 'b' era stata definita prima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se si esce dal notebook, dopo averlo salvato ovviamente, si rientra e si eseguono le celle nell'ordine, tutto funziona (perchè la variabile *b* è ancora in memoria). Se invece si esce da Jupyter e poi si rientra, si apre il file e si eseguono le celle in ordine, la cella *axb* dà giustamente errore!<br>\n",
    "Cioè, se la sequenza delle celle non è corretta, il notebook può apparentemente funzionare anche dopo la sua chiusura e riapertuura (grazie alla persistenza in memoria delle variabili allocate). Ci accorgeremo dell'errore del notebook, con sorpresa, solo dopo aver chiuso e riattivato Jupyter!<br>\n",
    "Ciò vale per tutti gli oggetti *python*, ad esempio anche con le funzioni (definite PRIMA di essere usate?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alcune volte, inoltre, l'output delle celle sembra persistere in memoria anche se Jupyter è chiuso e riavviato. Tali oggetti non sono visibili nel Variable Inspector, che non elenca nessun oggetto in questi casi, tuttavia le celle non sono eseguite! Cioè non ci si rende  conto dell'ordine errato delle celle sino a chè non si spegne e riavvia il computer! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insidie di Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doppio save??\n",
    "Attenzione: Jupyter **non ha controllo di versione**. Pertanto:\n",
    "* se aprite la versione A del notebook e la modificate (al tempo t0, senza fare save o checkpoint automatico!) \n",
    "* e poi involontariamente (capita!) aprite una seconda versione B del <u>medesimo</u> notebook (sun altro tab del browser) e la modificate al tempo t1 (>t0)\n",
    "* poi salvate la versione B\n",
    "* ed infine (erroneamente) salvate la versione A, le modifiche fatte al tempo t1 sulla versione B sono perse perchè sovrascritte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cut & Paste dentro una cella??\n",
    "Provate sulla cella di prova sottostante. Se selezionate Parte 2 e poi premete l'icona in alto a sinistra (le forbici) --> perdete l'intera cella.<br>\n",
    "Le icone del menù in alto lavorano sulle **celle intere**. Per lavorare con le parti di una cella usate i comandi del menu **tasto destro del mouse**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parte 1.\n",
    "\n",
    "Parte 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testo in rosso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'editor visualizza in rosso le variabili (ed i commenti) che non sono indentati secondo gli standard (4 spazi). Vedi [qui](https://stackoverflow.com/questions/35330872/why-are-some-variables-and-comments-in-my-ipython-notebook-red)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Il dataset iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Toy dataset forniti da scikit-learn](https://scikit-learn.org/stable/datasets/toy_dataset.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets # sklearn is part of scikit-learn (so 46113732)\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris() # è un oggetto scikit-learn, inclusivo di documentazione. Non è ancora un df pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "print(iris.data) # le variabili di previsione \n",
    "                 # feature, predittori, variabili indipendenti, dimensioni, assi di analisi, colonne, attributi, proprietà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target # i VALORI della variabile risposta (per ogni riga)\n",
    "            # risposta, output, variabile dipendente, ecc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names # i valori DISTINTI del target\n",
    "                  # The classes are already converted to integer labels where 0=Iris-Setosa, 1=Iris-Versicolor, \n",
    "                  # 2=Iris-Virginica. (PML notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print(iris.target_names) # come sempre la funzione 'print' cambia (può cambiare) l'output. \n",
    "                         # 'print' di python3 (funzione) è diversa da python2 (istruzione)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(iris.DESCR) # la descrizione inclusa nel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Utente\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\iris.csv'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attenzione:\n",
    "type(iris) # un bunch dataset.\n",
    "           # 'type' fornisce la classe dell'oggetto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Conversione di un dataset scikit-learn in un df pandas](https://stackoverflow.com/questions/38105539/how-to-convert-a-scikit-learn-dataset-to-a-pandas-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= list(iris['feature_names']) + ['target'])\n",
    "print(df)\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes  # This returns a Series with the data type of each column.\n",
    "           # 'dtype' sta per 'data-type'.\n",
    "           # --> Non confondere con 'type(df)'' che fornisce il data-type dell'oggetto 'df' nel suo insieme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(['number']) # utile metodo per selezionare tutte e sole le variabili NUMERICHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(['object']) # utile metodo per selezionare tutte e sole le variabili STRINGA (object);\n",
    "                             # --> nessuna, in questo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(['datetime']) # utile metodo per selezionare tutte e sole le variabili DATETIME; \n",
    "                             # --> nessuna, in questo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(['timedelta']) # utile metodo per selezionare tutte e sole le variabili TIMEDELTA;\n",
    "                             # --> nessuna, in questo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(['category']) # utile metodo per selezionare tutte e sole le variabili CATEGORICHE;\n",
    "                               # nessuna, in questo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df # meglio che 'print(df) in questo caso'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.DataFrame()) # il DF è la struttura di pandas più importante che estende le serie, ci permette di importare i dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Plot di iris](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() # funzioni statistiche (solo per le variabili numeriche, che sono comunque escluse automaticamente dal metodo);\n",
    "              # questo metodo interpreta la variabile 'target' come numerica;\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiamo i nomi delle variabili (senza i cm) - con 'inplace=True'\n",
    "df.rename(columns={'sepal length (cm)': 'sepal length', 'sepal width (cm)': 'sepal width','petal length (cm)': 'petal length','petal width (cm)': 'petal width'},inplace=True)\n",
    "# Notare l'argomento 'inplace=True.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns # i nuovi nomi delle variabili (per lo standard PEP8 in lettere minuscole ed underscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index # l'indice numerico creato automaticamente da pandas (la prima colonna in neretto nel display di prima)\n",
    "# attenzione: un df pandas NON è una tabella relazionale con chiavi primarie, secondarie ed esterne; singole e composte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_index() --> Utile quando si vuole sostituire all'indice numerico progressivo automaticamente creato da 'pandas' \n",
    "# (la prima colonna) un altro indice su una certa colonna.\n",
    "\n",
    "df.set_index('sepal length',inplace=True)  # poco sensato qui\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)  # senza l'argomento 'inplace' il reset index vale solo per l'output della cella,\n",
    "                              # ma non persistente nell'oggetto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per indici multipli c'è la funzione di pandas 'MultiIndex'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()  # --> range index, non-null values, data types, memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame({'Variabili': df.columns})) # mix di testo e dati (DMforBA, python ed., p. 167)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape[0])\n",
    "print(df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by = 'sepal length',ascending=True) # senza 'inplace=True', altrimenti diventa permanente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by = 'sepal length', ascending = False) # in modo discendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['sepal length']) # un semplice modo per liberarsi di una o più colonne del df.\n",
    "                                  # inplace=True lo rende persistente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby(by='sepal length'))            # senza metodi di aggregazione, la groupby si limita a creare l'oggetto\n",
    "                                                # raggruppato\n",
    "print(df.groupby(by='sepal length').count())    # 'count()' è una delle molte funzioni di aggregazione.\n",
    "                                                # Per maggiori dettagli ed esempi sulla funzione 'groupby' vedi Corso TS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is designed to work only on a single core. Pandas cannot utilize the multi-cores available on the system.<br>\n",
    "However, the *cuDF* library aims to implement the Pandas API on the GPU; *Modin* as well as *Dask Dataframe library* provides parallel algorithms around the Pandas API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## il subsetting di pandas\n",
    "**accesso**: *slicing*, *subsetting*, *indexing*: termini sostanzialmente equivalenti per indicare un sottoinsieme di righe e/o colonne.\n",
    "\n",
    "diversi metodi:\n",
    "* il metodo di BASE (so 16096627)\n",
    "* loc\n",
    "* iloc\n",
    "\n",
    "Secondo DMBA: il metodo *loc* è più generale e permette di accedere alle righe usando le label; il metodo *iloc*, d'altra parte, permtte di usare solo numeri interi.<b>\n",
    "\n",
    "Ci sono alcune inconsistenze; ad esempio: *iloc* esclude l'estremo superiore del range 0:9 (secondo la convenzione generale di Python); al contrario, il metodo *loc* lo include.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df # per tenerlo in vista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniziamo con il metodo di base.\n",
    "# estraiamo i primi tre casi per intero  (so 16096627)\n",
    "df[:3] # da 0 a lenght-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[4:] # i casi dal quinto fino alla fine del dataset. \n",
    "# --> Ci sono delle righe con ... (non visualizzate) perchè è un subset troppo grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[4:]     # ovviamente nelle copie dati, sono copiati TUTTE le righe.\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1:3] # da 1 a lenght-1 (so 16096627)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[::2] # i casi uno sì ed uno no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> non è possibile selezionare una sola riga! è una inconsistenza del metodo di base, occorre usare il metodo .iloc, vedi \n",
    "#     più avanti (so 16096627)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con [label] selezioniamo una colonna come SERIE (un'altra struttura di pandas). Notare l'uso di [] anzichè di [[]].\n",
    "df['sepal length'] # label colonna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Series vs. Dataframe in pandas](https://stackoverflow.com/questions/38105539/how-to-convert-a-scikit-learn-dataset-to-a-pandas-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['sepal length']) # è una Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df['sepal length'])              # la serie (indice e valori)\n",
    "print('\\n',df['sepal length'].values)  # i valori della serie\n",
    "print('\\n',df['sepal length'].index)   # il range dell'indice della serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.Series())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['sepal length'])    # lunghezza della serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sepal length'].shape   # la stessa cosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:2].shape               # si agisce sul dataframe che ha comunque due dimensioni (è solo un subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['sepal length']] # estrazione colonna per nome (con la modalità dataframe: [[]]);\n",
    "                     # riporta solo le prime ed ultime osservazioni;\n",
    "                     # sia l'indice che i valori della colonna del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df[['sepal length']]) # oggetti differenti (dataframe o series, in questo caso) hanno metodi differenti associati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['sepal length']].values) # estrazione colonna per nome (con la modalità dataframe: [[]]);\n",
    "                                   # riporta TUTTE le osservazioni (senza interruzioni ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sepal_length            # comoda alternativa se il nome colonna NON contiene spazi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['sepal length']].index)  # il range dell'indice della serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['sepal length']].head(2) # solo le prime due righe (di quella colonna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['sepal length', 'sepal width']] # 2 colonne (in modalità dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sepal length'][2] # selezioniamo un caso di una variabile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sepal length'][0:10] # le prime 10 righe di una determinata colonna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['sepal length', 'sepal width']][0:2] # 2 colonne (in modalità dataframe) delle prime due righe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['sepal length', 'sepal width']][0] # NON funziona! (come detto prima: col metodo base di subsetting di Pandas NON è \n",
    "                                       # possibile estrarre una sola riga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc vs iloc, i due metodi di accesso (slicing) a righe e colonne dei dataframe di pandas (DMforBA, p. 25);\n",
    "# - il metodo loc è più generale e permette di accedere alle righe tramite le label; loc include l'ultimo elemento del range\n",
    "#   specificato, a differenza di numpy e del metodo iloc;\n",
    "# - il metodo iloc permette di accedere solo tramite gli interi\n",
    "print(df.loc[0:3])    # le prime 4 righe (da 0 a 3)\n",
    "print(df.iloc[0:3])   # le prime 3 righe (da 0 a 2) - comportamento standard di python\n",
    "\n",
    "# NB. loc ed iloc, se la selezione è resa persistente, fanno perdere i nomi delle variabili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selezione di un solo caso (non era possibile col metodo base di pandas) \n",
    "df.iloc[0,:] # tutte le colonne del caso 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .iloc (usa l'indice intero riga colonna) - sintassi COERENTE con quella di numpy per il subsetting delle array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,2] # tutti i valori della colonna 3 (l'opposto di prima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0,2] # un SINGOLO elemento: df.iloc[row_indexer,column_indexer]df1.iloc[0,2] # un SINGOLO elemento: df.iloc[row_indexer,column_indexer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0,20] # IndexError --> out of bounds (errore voluto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0:2,2] # un insieme di righe (consecutive) da 0 a length-1 (solo colonna 2, cioè petal length, la terza!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:2,2]  # stessa cosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0,0:3] # un insieme di colonne (consecutive) del caso 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[[4,3,0],2] # un insieme di righe non consecutive (solo colonna 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[1] # un solo indice è interpretato come indice di riga (a differenza di altri linguaggi di ML, ad es. R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uso degli operatori (nel subsetting) --> condizioni di estrazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['sepal length'] > 7] # tutti i casi con height > 7 # subsetting di base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternativa molto usata (ad esempio da Hwang) con subsetting di tipo .loc\n",
    "df.loc[df['sepal length'] > 7] # tutti i casi con height > 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# come sopra ma con =:\n",
    "df.loc[df['sepal length'] == 7.1] # tutti i casi con height = 7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenazione di colonne non-consecutive in un nuovo df (DMforBA, p. 24)\n",
    "pd.concat([df.iloc[4:6,0],df.iloc[4:6,3]],axis=1) # combina la colonna 1 dei casi 5 e 6 con la colonna 4 dei medesimi casi.\n",
    "                                                  # 'axis' specifica la dimensione lungo la quale la concatenazione avviene.\n",
    "                                                  # 0 righe - 1 colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting BOOLEANO\n",
    "sample_arr = [True, False]\n",
    "bool_arr = np.random.choice(sample_arr, size=df.shape[0])\n",
    "print(bool_arr)\n",
    "df[bool_arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vedi anche [questo post](https://www.kdnuggets.com/2019/06/select-rows-columns-pandas.html?__s=o7u740my87lkp9nksy5d&utm_source=drip&utm_medium=email&utm_campaign=Building+an+image+classifier&utm_content=Building+an+image+classifier) (*How to Select Rows and Columns in Pandas Using [ ], .loc, iloc, .at and .iat*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *numpy*\n",
    "Il pacchetto per il calcolo scientifico di base, estendibile con il pacchetto gemello ***scipy*** che si appoggia su numpy stesso. <br>Il principale oggetto di numpy sono le **ndarray** (n-dimensional array), più semplicemente chiamate <u>array</u>. Sono usate per rappresentare <u>vettori</u> (1D), <u>matrici</u> (2D) e <u>tensori</u> (3D+). Gli 0D-array sono <u>scalari</u>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np   # np è l'abbreviazione convenzionale per numpy \n",
    "import scipy as sp   # sp è l'abbreviazione convenzionale per scipy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creazione di array numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(2)                   # --> si ottiene uno scalare (0D-array)\n",
    "np.array(2,ndmin=1)           # --> si ottiene un vettore (1D-array) \n",
    "np.array(2,ndmin=2)           # --> si ottiene una matrice (2D-array)\n",
    "                              # in tutti e tre i casi l'array è costituita da un SOLO numero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1,2,3],ndmin=0)     # un vettore con tre componenti\n",
    "np.array([1,2,3])             # idem \n",
    "                              # cioè, indipendentemente da 'ndmin', ora, per crare un vettore si elencano dentro le \n",
    "                              # parentesi quadre tutti gli elementi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1,2,3],ndmin=2)     # una matrice 1x3; è necessario per moltiplicare il vettore (colonna) con una matrice\n",
    "np.shape(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[1,2,3],\n",
    "          [4,5,6]],\n",
    "         ndmin=1)             # una matrice 2x3\n",
    "\n",
    "np.array([[1,2,3],\n",
    "          [4,5,6]])           # una matrice 2x3\n",
    "                              # cioè, indipendentemente da 'ndmin', per crare una matrice si elencano dentro le parentesi \n",
    "                              # quadre tutti i vettori [..]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3],\n",
    "          [4,5,6]],\n",
    "         ndmin=3)             # ??\n",
    "np.shape(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni di creazione utili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "np.identity(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "m = 5\n",
    "np.eye(n,m,k=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(n,m)       # una matrice di numeri pseudo-casuali (ognuno estratto da una distribuzione uniforme standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metodi ed attributi di un'array numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3],\n",
    "          [4,5,6]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape         # le dimensioni dell'array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.ndim          # il numero di dimensioni dell'array\n",
    "len(a.shape)    # la stessa cosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.size          # il numero totale di elementi dell'array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.T             # l'array trasposta (righe e colonne invertite); non ha effetto se a è 0D oppure 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.copy()    # crea una NUOVA array che è la copia di a.\n",
    "                # [b = a --> si continua a lavorare sulla STESSA array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.nonzero()     # restituisce un'array di indici che corrispondono agli elementi non-zero di a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.sum(axis=0)   # somma lungo l'asse indicato (qui le colonne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.sum(axis=1)   # somma lungo l'asse indicato (qui le righe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.min(axis=1)   # gli elementi minimi delle due righe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.max(axis=0)   # gli elementi massimi delle tre colonne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting di un'array numpy 1D\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting di un'array numpy 2D\n",
    "Da [questo](https://stackoverflow.com/questions/30917753/subsetting-a-2d-numpy-array) post stackoverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 87,  96,  70],\n",
       "       [100,  87,  90],\n",
       "       [ 94,  77,  90],\n",
       "       [100,  81,  82]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i dati\n",
    "voti = np.array([[87,96,70], [100,87,90], \n",
    "                 [94,77,90],[100,81,82]])\n",
    "voti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selezione di un elemento:\n",
    "voti[0,1]       # prima riga, seconda colonna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100,  87,  90])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voti[1]         # la seconda riga (per default, numpy considera l'unico indice come indice di riga, come fa anche il \n",
    "                # metodo iloc di pandas - vedasi sopra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 87,  96,  70],\n",
       "       [100,  87,  90]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voti[0:2]       # le prime due righe (sequenziali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100,  87,  90],\n",
       "       [100,  81,  82]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voti[[1,3]]     # righe NON sequenziali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voti[:,0]       # la prima colonna (':' indica tutte le righe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voti[:,1:3]     # due colonne in sequenza (la seconda e la terza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "voti[0:,[0,2]]  # due colonne NON in sequenza (la prima e la terza) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora un altro esempio da [questo](https://stackoverflow.com/questions/30917753/subsetting-a-2d-numpy-array) post stackoverflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creazione dei dati (l'array numpy 2D 'a')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "a = np.arange(100)\n",
    "a.shape = (10,10)\n",
    "\n",
    "# scritto meglio:\n",
    "a = np.arange(100).reshape(10,10)\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0] # la prima riga dell'array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBIETTIVO più generale: subsetting di righe e colonne di a, specificate dai vettori n1 ed n2:\n",
    "\n",
    "n1 = range(5)\n",
    "n2 = range(5)\n",
    "\n",
    "# scritto meglio:\n",
    "n1, n2 = np.arange(5), np.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[n1,n2] # i primi 5 elementi della diagonale principale;\n",
    "         # NON è il risultato attteso (che è invece una subarray n1*n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# il risultato attteso (l'estrazione della subarray 5*5 in alto a sx), tramite creazione di una nuova array 'b'\n",
    "b = a[n1,:]     \n",
    "b = b[:,n2]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorremmo una 'VISTA' dell'array 'a', senza la creazione di una nuova array 2D.\n",
    "\n",
    "# RISPOSTA più votata:\n",
    "# There's a big difference between \"fancy\" indexing (i.e. using a list/sequence) and \"normal\" indexing (using a slice).\n",
    "# The underlying reason has to do with whether or not the array can be \"regularly strided\" (percorsa), and therefore \n",
    "# whether or not a copy needs to be made. Arbitrary sequences therefore have to be treated differently, if we want to \n",
    "# be able to create \"views\" without making copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not what you want\n",
    "b = a[n1, n2]  # array([ 0, 11, 22, 33, 44])\n",
    "\n",
    "# What you want, but only for simple sequences\n",
    "# Note that no copy of *a* is made!! This is a view.\n",
    "b = a[:5, :5]\n",
    "print(b)\n",
    "\n",
    "# What you want, but probably confusing at first. (Also, makes a copy.)\n",
    "# np.meshgrid and np.ix_ are basically equivalent to this.\n",
    "b = a[n1[:,None], n2[None,:]]\n",
    "print('\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# un'altra soluzione, con la funzione 'np.ix'\n",
    "a[np.ix_(n1, n2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# un'altra soluzione ancora\n",
    "a = np.arange(100).reshape(10,10)\n",
    "subsetA = [1,3,5,7]\n",
    "a[subsetA].T[subsetA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fancy indexing with 1D sequences is basically equivalent to zipping them together and indexing with the result.\n",
    "\n",
    "print (\"Fancy Indexing:\")\n",
    "print (a[n1, n2])\n",
    "\n",
    "print ('\\n',\"Manual indexing:\")\n",
    "for i, j in zip(n1, n2):\n",
    "    print (a[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, if the sequences you're indexing with (will?, ndt) match the dimensionality of the array you're indexing \n",
    "# (2D, in this case), the indexing is treated differently. Instead of \"zipping the two together\", numpy uses the \n",
    "# indices like a mask.\n",
    "# In other words, a[[[1, 2, 3]], [[1],[2],[3]]] is treated completely differently than a[[1, 2, 3], [1, 2, 3]], \n",
    "# because the sequences/arrays that you're passing in are two-dimensional.\n",
    "print(a[[[1, 2, 3]], [[1],[2],[3]]])\n",
    "print('\\n',a[[1, 2, 3], [1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be a bit more precise:\n",
    "a[[[1, 2, 3]], [[1],[2],[3]]]\n",
    "# is treated exactly like:\n",
    "i = [[1, 1, 1],\n",
    "     [2, 2, 2],\n",
    "     [3, 3, 3]]\n",
    "j = [[1, 2, 3],\n",
    "     [1, 2, 3],\n",
    "     [1, 2, 3]]\n",
    "a[i, j]\n",
    "# In other words, whether the input is a row/column vector is a shorthand for how the indices should repeat in the \n",
    "# indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertire array numpy\n",
    "Da ND ad 1D: vedi [questo post so](https://stackoverflow.com/questions/13730468/from-nd-to-1d-arrays).<br>\n",
    "Ad esempio: *np.reshape(X,-1)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertire un df *pandas* in un'array *numpy*\n",
    "Vedi [questo post](https://stackoverflow.com/questions/13187778/convert-pandas-dataframe-to-numpy-array) di *stack overflow*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values\n",
    "df_to_numpy()  # meglio! vedi il post\n",
    "to_numpy()     # per consistenza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcoli numerici in *pandas* \n",
    "*pandas* è ottimo per la gestione dei dataframe, *numpy* per i calcoli numerici.<br>\n",
    "Diverse funzioni di *pandas* che richiedono **calcoli numerici** (ad es. standardizzazione, calcolo correlazione, ecc) ricevono in ingresso un *dataframe* e restituiscono *un'array* di *numpy* (non necessariamente da riconvertire in *pandas*).<br>\n",
    "Occorre perciò <u>conoscere i data-type di entrambi i package</u> (*pandas* e *numpy*) e sapere come <u>convertirli</u> da uno all'altro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'API di scikit-learn\n",
    "Uno dei maggiori benefici di scikit-learn.<br>\n",
    "Un'interfaccia molto ben documentata on-line.<br>\n",
    "Un'interfaccia semplice e consistente per tutti i metodi di scikit-learn, detti 'estimators'.<br>\n",
    "4 metodi in sequenza: costruttore, fit, predict, transform (per metodi non-supervisionati).<br>\n",
    "input/output sono array numpy o sparse matrix (scikit-learn è costruito sopra numpy e scipy, infatti).<br>\n",
    "probability classification con 'predict_proba'.<br><br>\n",
    "\n",
    "![API di scikit-learn](sklearn_elements.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[API Reference](https://scikit-learn.org/stable/modules/classes.html)<br><br>\n",
    "[Common pitfalls and recommended practices (in particolare: data leakage)](https://scikit-learn.org/stable/common_pitfalls.html)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u>Dalle FAQ di scikit-learn:</u>**<br><br>\n",
    "**How can I load my own datasets into a format usable by scikit-learn?**\n",
    "Generally, scikit-learn works on any numeric data stored as numpy arrays or scipy sparse matrices. Other types that are convertible to numeric arrays such as pandas DataFrame are also acceptable. *[nota MIA: pandas è costruito sopra numpy! - PML, p. 16].* For more information on loading your data files into these usable data structures, please refer to 'loading external datasets'.\n",
    "\n",
    "**Why does Scikit-learn not directly work with, for example, pandas.DataFrame?**\n",
    "The homogeneous NumPy and SciPy data objects currently expected are most efficient to process for most operations. Extensive work would also be needed to support Pandas categorical types. Restricting input to homogeneous types therefore reduces maintenance cost and encourages usage of efficient data structures.\n",
    "\n",
    "**Why do categorical variables need preprocessing in scikit-learn, compared to other tools?**\n",
    "Most of scikit-learn assumes data is in NumPy arrays or SciPy sparse matrices of a single numeric dtype. These do not explicitly represent categorical variables at present. Thus, unlike R’s data.frames or pandas.DataFrame, we require explicit conversion of categorical features to numeric values, as discussed in Encoding categorical features. See also Column Transformer with Mixed Types for an example of working with heterogeneous (e.g. categorical and numeric) data.\n",
    "\n",
    "**How do I deal with string data (or trees, graphs…)?**\n",
    "scikit-learn estimators assume you’ll feed them real-valued feature vectors. This assumption is hard-coded in pretty much all of the library. However, you can feed non-numerical inputs to estimators in several ways.\n",
    "\n",
    "Gli argomenti degli stimatori (*estimator*) che terminano per '-' <u>per convenzione</u> sono stati creati dai metodi successivi al creatore dell'istanza (Raschka, p. 26). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![API di scikit-learn](skln_API_transformation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![API di scikit-learn](skln_API_classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Persistenza dei modelli](https://scikit-learn.org/stable/modules/model_persistence.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "input_data = np.array(\n",
    "[[2.1, -1.9, 5.5],\n",
    " [-1.5, 2.4, 3.5],\n",
    " [0.5, -7.9, 5.6],\n",
    " [5.9, 2.3, -5.8]]\n",
    ")\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# il seguente comando apre un'ampia finestra informativa sull'oggetto il cui nome è prima del ?\n",
    "input_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# array alternativo (di test): NON USARLO nel seguito dello notebook come esempio!\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "input_data = np.array(\n",
    "[[1,2,3],\n",
    " [4,5,6],\n",
    " [7,8,9],\n",
    " [10,11,12]]\n",
    ")\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.shape # in questo consistenza con pandas nel nome del metodo 'shape'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.mean() # la media totale su tutti i valori. E' un metodo associato ad un oggetto nd-array di numpy.\n",
    "                  # la documentazione in linea (shift+tab dentro le parentesi) riporta, per questa ed altre funzioni numeriche:\n",
    "                  # 'Refer to `numpy.mean` for full documentation.'. Cioè rimanda alla funzione. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sepal length'].mean() # consistenza tra pandas e numpy sul nome di questo metodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nota bene: senza parentesi.\n",
    "input_data.mean # specifica il metodo, implementato con una funzione (vedi anche help), come detto prima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(input_data.mean(),2) # con arrotondamento a due cifre decimali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.mean(axis=0) # le medie sull'asse X (cioè delle colonne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_data.mean(axis=0)) # una migliore formattazione?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.mean(axis=1) # le medie sull'asse Y (cioè delle righe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.std() # la deviazione standard totale su tutti i valori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.std(axis=0) # le dev std delle colonne\n",
    "                       # attenzione: le dev.std fornite da questa funzione sono biased, cioè i SS sono\n",
    "                       # divisi per n, e non per (n-1) come in R. [Con pochi dati ciò è discutibile] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(np.std) # la funzione 'np.std' (non il metodo!!) ha l'argomento 'ddof' per impostare il valore del divisore\n",
    "             # 'ddof' means Delta Degrees of Freedom.  The divisor used in calculations is ``N - ddof``, where ``N`` \n",
    "             # represents the number of elements. By default `ddof` is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(input_data,axis=0,ddof=1) # le dev std delle colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.cov(input_data,rowvar=False)) # matrice delle varianze / covarianze.\n",
    "                          # è una funzione; il metodo .cov non è disponibile.\n",
    "                          # attenzione: il default è 'rowvar' = True (\"Each row represents a variable, and each column a single \n",
    "                          # observation of all those variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.corrcoef(input_data,rowvar=False)) # matrice delle correlazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_data = preprocessing.scale(input_data) # la funzione 'scale' fa SIA centering CHE rescaling;\n",
    "                                           # notare i 3 booleani a True per default.\n",
    "                                           # usa uno stimatore BIASED della dev.std. Dall'help in linea, infatti:\n",
    "                                           # \"We use a biased estimator for the standard deviation, equivalent to\n",
    "                                           # 'numpy.std(x, ddof=0). Note that the choice of 'ddof' is unlikely to\n",
    "                                           # affect model performance.\"\n",
    "                                           # la funzione 'scale' NON ha l'argomento 'ddof'!\n",
    "            \n",
    "print(std_data)                            # --> standardizzare significa prendere le SD come unità di misura.\n",
    "print(input_data)                          # per confronto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**standardizzazione**: 2 passi:\n",
    "* centratura, aka centering oppure mean removal\n",
    "* (re)scaling, cioè dev. std tutte a 1\n",
    "\n",
    "**(Xi - mu) / sigma**. <br>\n",
    "Se 'with_mean' = False --> mu = 0;<br>\n",
    "Se 'with_std' = False --> sigma = 1.<br>\n",
    "dove mu e sigma sono di colonna (cioè calcolate su tutte le righe, per ogni colonna)\n",
    "\n",
    "Vantaggi ed applicazioni.<br>\n",
    "Considerazioni numeriche (Brandimarte I, p. 217).<br><br>\n",
    "--> I modelli lineari (ad es.Regressione Lineare, Regressione Logistica) sono molto sensibili alla trasformazione lineare, \n",
    "    <u>non al rescaling</u> (ISLR, p. 217). Altri metodi di ML (Clustering, PCA, ecc), invece, sono molto sensibili al rescaling (da fare obbligatoriamente in questi casi, in modo preliminare).<br>\n",
    "    \n",
    "La standardizzazione dei dati, inoltre, facilita la convergenza dell'algoritmo di discesa del gradiente (*gradient descent*), \n",
    "spesso usato con funzioni di costo convesse (PML).<br>\n",
    "La standardizzazione non è una gausianizzazione.<br>\n",
    "Vedi anche DMforBA, p. 33."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(std_data.mean(axis=0)) # le medie delle colonne (standardizzate)\n",
    "                             # sono tutte e tre 0.\n",
    "                             # nei computer a doppia precisione (tutti, in pratica) e-16 è l'approssimazione digitale di 0\n",
    "print(input_data.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(std_data.std(axis=0)) # le medie delle colonne (standardizzate)\n",
    "                             # sono tutte e tre 0.\n",
    "                             # nei computer a doppia precisione (tutti, in pratica) e-16 è l'approssimazione digitale di 0\n",
    "print(input_data.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.11022302*10**-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota sulla rappresentazione interna dei numeri reali**<br>\n",
    "I numeri reali nei computer sono memorizzati separatamente, come mantissa ed esponente (in base 10).<br>\n",
    "Con la notazione scientifica, i numeri reali sono visualizzati in questo modo.<br>\n",
    "In una macchina a \"doppia precisione\" (la più comune), <u>l'esponente -16 associato ad una mantissa unitaria significa 0</u>.<br>\n",
    "Ovviamente, se, a seguito dei calcoli, un numero ha mantissa 0.01 ed esponente -16, è rappresentato più correttamente dal<br>\n",
    "computer come mantissa 1 ed esponente -18.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3) # imposta la precisione dell'output della mantissa (se tutte le cifre decimali danno fastidio)\n",
    "print(std_data.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=False) # sopprime l'uso della notazione scientifica per piccoli numeri:\n",
    "print(std_data.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(std_data.std(axis=0)) # le dev std delle colonne (standardizzate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizzazione fattibile anche così:\n",
    "(input_data - input_data.mean(axis=0)) / input_data.std(axis=0) # ricordarsi di 'axis'\n",
    "# --> illustra il funzionamento VETTORIALE di numpy e pandas (non serve un loop). R e Matlab funzionano così, i linguaggi 3GL, no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centered_data = preprocessing.scale(input_data,with_std=False) # per centrare solo\n",
    "centered_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(centered_data.mean(axis=0)) # le medie di colonne sono giustamente a zero (circa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(centered_data.std(axis=0)) # le dev std originali\n",
    "                                 # la centratura dati NON modifica la loro dispersione!\n",
    "print(input_data.std(axis=0))    # per confronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = preprocessing.scale(input_data,with_mean=False) # per riscalare solo (no centratura)\n",
    "scaled_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaled_data.std(axis=0)) # le dev std delle colonne sono tutte a 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaled_data.mean(axis=0)) # le medie delle colonne non sono perchè non ho richiesto la centratura\n",
    "# (ma non sono quelle originali, ovviamente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spesso utile \"normalizzare\" i dati in un determinato intervallo, ad es. 0-1\n",
    "data_normalizer = preprocessing.MinMaxScaler(feature_range=(0,100)) # costruzione del normalizzatore \n",
    "data_normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = data_normalizer.fit_transform(input_data) # applicazione del metodo del normalizzatore ai dati di input\n",
    "normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_2 = np.array([[4,5,6]]); print(input_data_2)\n",
    "data_normalizer = preprocessing.MinMaxScaler(feature_range=(0,100)) # costruzione del normalizzatore \n",
    "normalized_data = data_normalizer.fit_transform(input_data_2) # applicazione del metodo del normalizzatore ai dati di input\n",
    "normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la normalizzazione implementata da 'MinMaxScaler' è questa (che NON è la standardizzazione):\n",
    "(input_data_2 - input_data_2.min(axis=1)) / (input_data_2.max(axis=1) - input_data_2.min(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la normalizzazione implementata da 'MinMaxScaler' è questa (che NON è la standardizzazione):\n",
    "(input_data_2 - input_data_2.min) / (input_data_2.max - input_data_2.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarizzazione dei dati: usata in alcuni algoritmi, ad esempio nei motori di raccomandazione \n",
    "data_binarized = preprocessing.Binarizer(threshold=3).transform(input_data)\n",
    "data_binarized # tutti i valori SOPRA il threshold sono convertiti ad 1, tutti quelli SOTTO sono convertiti a 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow 2** permette di costruire *data pipeline* (per la trasformazione dei dati) molto efficienti per:\n",
    "* centrare i dati\n",
    "* scalare i dati\n",
    "* aggiungere rumore per aumentare il training dataset e prevenire l'overfitting (*data augmentation*, molto usata per costruire le reti neurali di classificazione immagini). <br>\n",
    "\n",
    "Vedasi Python ML, p. 436."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Riproducibilità dei risultati.\n",
    "Nel ML (e non solo) serve un generatore di numeri casuali. <br>\n",
    "E' utile quando c'è un campionamento di dati oppure c'è un *tie* (legame) nei dati da risolvere (ad es. in kNN - vedi cap. B). <br>\n",
    "Utile anche nelle simulazioni di Monte Carlo. <br>\n",
    "\n",
    "Non è facile da realizzare. Si parla quindi di \"pseudo\"-generazione. <br>\n",
    "Si parte da un \"seme\". <br>\n",
    "\n",
    "Metodo standard: Linear Congruential Generator (LCG), che genera delle variabili uniformi standard, <br>\n",
    "cioè variabili equiprobabili in (0,1). <br>\n",
    "\n",
    "Metodo della trasformata inversa.<br>\n",
    "\n",
    "Brandimarte 1 (pp. 451) e Brandimarte 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in scikit-learn spesso realizzata con l'argomento 'random-state=k'.\n",
    "\n",
    "# 'random_state' can be 0 or 1 or any other integer. It should be the same value if you want to validate your processing \n",
    "# over multiple runs of the code; \n",
    "# the 'random_state' parameter is used for initializing the internal random number generator.\n",
    "# If random_state is an integer, then it is used to seed a new RandomState object.\n",
    "# if random_state is None or np.random, then a randomly-initialized RandomState object is returned.\n",
    "\n",
    "# in numpy l'inizializzazione del generatore di (pseudo) numeri casuali è tramite la funzione 'random.seed'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(5,4) # matrice casuale di numeri tra 0 e 1 - ogni volta diversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "np.random.rand(5,4) # matrice casuale di numeri tra 0 e 1 - sempre la stessa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Importanti funzioni ulteriori](https://docs.python.org/3/library/random.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilità di Python<br>\n",
    "Molte utilità sono disponibili nel package *dmba*, che è presente nel *PPI (Python Package Index)* all'indirizzo: <u>https:// pypi.org/ project/ dmba</u>. Installare il package con: *pip install dmba* (da un prompt anaconda) e poi fare: *import dmba* da una cella Jupyter. Il codice sorgente è mantenuto su GitHub a: *https:// github.com/gedeck/dmba*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vedi libro \"Introduzione a Python\", pp. 313-315."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "634.594px",
    "left": "1084px",
    "top": "134.125px",
    "width": "309.857px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "192.188px",
    "left": "271.55px",
    "right": "20px",
    "top": "181px",
    "width": "357.763px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
